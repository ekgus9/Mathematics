# 13 주성분분석



### 13-2 PCA 개관과 PC의 유도



* 주성분 분석 PCA : 차원 축소 기법 (고윳값 분해나 특이값 분해 사용) / 기존의 병수를 일차 결합하여 선형 연관성이 없는 새로운 변수, 즉 주성분을 만들어 낸다. (주선분 PC1이 원데이터의 분포를 가장 많이 보존)
* mean-centered 행렬 : p개의 확률변수에 대한 공분산 행렬(시그마 = 1/n X^T X)에서 X를 n * p의 데이터 행렬이라 할 때, X를 센터링(각 열들의 평균을 구해서 각 열에서 그 평균을 뺌 -> 각 열의 평균 0)
* 차원 축소 : 공분산행렬로부터 얻은 정보를 최대한 보존하는 더 적은 개수의 새로운 변수들을 찾아야 한다. 
* 라그랑주의 승수법 : var(zk) = 시그마에서 k번째로 큰 고윳값



### 13-3 PC 스코어



X : 센터링된 n * p의 데어터 행렬, n : 표본 개수, p : 확률변수 개수



공분산 행렬은 p * p 대칭행렬이고, 다음과 같이 직교 대각화 가능하다. 

```
V A V^T 
(A : 시그마의 고윳값을 크기 순서대로 주대각선 성분에 배열한 대각행렬, V : 그에 대응하는 정규직교인 교유벡터를 열벡터로 하는 직교 행렬)
```

<Xv,Xv> >= 0 (||Xv|| = ||V|| >= 0) 이므로 **시그마**는 **양의 준정부호 행렬**이고, **고유벡터**는 **주축 또는 주방향**이라 부른다.



* 주성분 z1, z2, ... , zn 을 벡터로, 그리고 각 확률변수 xi에 원데이터를 대입하면 **Z = XV** 



(Z의 i번째 데이터 좌표 : XV의 i번째 **행벡터**, j번째 PC score 주성분 점수는 XV의 j번째 **열벡터**)



![image](https://user-images.githubusercontent.com/89879599/150476022-bb1e8b72-7e81-4b0c-a7a5-ec170823be06.png)



* 특이값 분해 truncated SVD 로 원래 행렬 Xp과 Xk의 차이 최소화 

```
X = U S V^T (-> PC score 구한다.)
```

```
결과
1. V가 열벡터의 주축이 된다.
2. 공분산 행렬의 고윳값은 대응하는 PC의 분산이 된다. 
3. 데이터를 p에서 k차원으로 줄이기 위해 U의 처음 k개의 열벡터 Uk와 S의 k번째 주 부분 행렬 Sk를 택하면 UkSk는 처음 k의 PC를 포함하는 n * k 행렬이된다. Zk = UkSk
4. Zk에 s1, s2, ... , sn에 대응되는 주축으로 이루어진 행렬 Vk^T를 곱하면, Xk = UkSk Vk^T는 처음 k개의 PC로 부터 원 데이터를 복원하도록 해주는 rank가 k인 n * p 행렬이 된다. 이는 X의 SVD에서 크기 순서대로 k개의 큰 특이값과 이에 대응되는 U와 V의 처음 k개의 열벡터를 택하는 것과 같다. (truncated SVD) 실제로 Xk는 rank가 k인 행렬 중 X에 가장 가까운 행렬이 된다.
```

### 13-4 PCA 예제

```
사람들이 새 컴퓨터를 선택할 때 관심을 갖는 아래 사항에 관하여 척도가 7점인 4문항의 리커트(Likert) 설문 조사를 16명에게 실시하고 얻은 데이터를 입력하여 데이터 행렬(data)을 생성한다. (1: 매우 그렇지 않다 – 7: 매우 그렇다)

     Price        가격이 저렴하다.

     Software     운영체제가 사용하려는 소프트웨어와 호환된다.

     Aesthetics    디자인이 매력적이다.

     Brand        유명 브랜드의 제품이다.
     
```

```
# 데이터 입력
Price <- c(6, 7, 6, 5, 7, 6, 5, 6, 3, 1, 2, 5, 2, 3, 1, 2) 
Software <- c(5, 3, 4, 7, 7, 4, 7, 5, 5, 3, 6, 7, 4, 5, 6, 3) 
Aesthetics <- c(3, 2, 4, 1, 5, 2, 2, 4, 6, 7, 6, 7, 5, 6, 5, 7) 
Brand <- c(4, 2, 5, 3, 5, 3, 1, 4, 7, 5, 7, 6, 6, 5, 5, 7) 
data <- data.frame(Price, Software, Aesthetics, Brand) 

# 데이터 행렬을 센터링 하여 mydata 행렬을 얻는다. (본 예제에서는 센터링 한 후에 다시 각 열을 해당 확률변수의 표준편차로 나누었다. 이를 scaling이라 하는데, 필요에 따라 scaling도 진행할 수 있다.)
# 표준화(centering, scaling) 작업
mydata <- scale(data, center = T, scale = T)  # 데이터 행렬을 센터링, scaling 한다.
print(mydata)

# mydata 행렬에 PCA를 진행한다. (R의 prcomp 명령어를 이용하였다.)
# PCA 진행. 앞에서 이미 센터링 scaling 하였음. retx는 PC score를 제공할지 여부
mypca <- prcomp(mydata, center = F, scale = F, retx = T)
print(summary(mypca))

# 주성분을 찾을 때 필요한 적재계수는 다음 명령어를 이용하여 확인할 수 있다. 이는 직교행렬 V에 해당한다. 
# loadings (V를 의미한다.)
print(mypca$rotation)

# 1~3번째 PC에서의 score는 다음 명령어를 이용하여 확인할 수 있다. 이는 행렬 Z=XV의 1~3번째 열벡터로 이루어진 행렬이다. 
# PC1~PC3 score. 1열부터 3열까지
print(mypca$x[,1:3])

# 앞서 PC1, PC2가 원 데이터의 84.8% 정도를 보존한다고 이해할 수 있으므로, 15% 정도의 정보는 잃어버리더라도, 
# PC1, PC2만 택하여 2차원 데이터로 차원을 줄 일 수 있다. 이를 활용하여 데이터를 시각화하거나 다른 데이터 분석기법을 적용할 수 있다. 
# 예를 들어, 다음과 같이 주어진 데이터를 개의 클러스터로 묶는 알고리즘인 K-means clustering을 진행할 수 있다. 
# 그리고 주성분의 고윳값에 대한 선 그림(line plot)인 scree plot을 활용하여 데이터의 차원을 줄일 때, 주성분을 몇 개로 선택할지 결정할 수 있다
# scree plot은 고윳값을 가장 큰 값에서부터 크기 순서대로 정렬하므로, 대개 기울기가 꺾이는 부분의 “elbow point” 왼쪽에 있는 성분까지 선택한다.
# 2차원으로 차원 축소 후(PC1과 PC2만 취하여) k-means clustering 진행
PC1 = mypca$x[,1] # 1열을 PC1라 함
PC2 = mypca$x[,2] # 2열을 PC2라 함

# PC1, PC2를 이용하여 클러스터의 개수(centers)를 4개로 하고
# 임의로 10번 시행(nstart)하여 가장 잘 clustering 하는 것을 찾음       	
myclust <- kmeans(mypca$x[,1:2], centers = 4, nstart = 10) 

# clustering 후 시각화 결과 보여줌
plot(PC1, PC2, xlab = "PC 1", ylab = "PC 2", type="p", col = myclust$cluster)

# scree 그래프 (분산을 나타낸다)
screeplot(mypca, type = "lines", col = 2)
dev.off()
```
